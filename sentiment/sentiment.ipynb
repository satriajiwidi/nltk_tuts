{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from stopwords import get_stopwords\n",
    "import corpus\n",
    "from collections import Counter\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n pos: 11\n",
      "n neg: 6\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "pos_data = corpus.pos\n",
    "neg_data = corpus.neg\n",
    "print('n pos:' , len(pos_data))\n",
    "print('n neg:', len(neg_data))\n",
    "\n",
    "# gabung semua data (pos dan neg)\n",
    "data = [] # teks dan label\n",
    "X = [] # teks saja\n",
    "y = [] # label/sentiment saja\n",
    "for (words, sentiment) in pos_data + neg_data:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3] \n",
    "    data.append((words_filtered, sentiment))\n",
    "    X.append(words_filtered)\n",
    "    y.append(sentiment)\n",
    "    \n",
    "index_pos = [(index) for index, value in enumerate(y) if value == 'positive']\n",
    "index_neg = [(index) for index, value in enumerate(y) if value == 'negative']\n",
    "\n",
    "pos_data = [X[index] for index in index_pos]\n",
    "pos_data_y = [y[index] for index in index_pos]\n",
    "neg_data = [X[index] for index in index_neg]\n",
    "neg_data_y = [y[index] for index in index_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenisasi\n",
    "# mentransformasi seluruh kalimat dalam corpus menjadi array dari\n",
    "# kata-kata\n",
    "def get_words(data):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in data:\n",
    "    \tall_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "# mendapatkan word features\n",
    "# atau kamus dari seluruh korpus\n",
    "# mengurutkan kata-kata dari frekuensi kemunculan tertinggi\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "# hapus stopwords dan tanda baca\n",
    "def remove_stopwords(words):\n",
    "    stopwords = get_stopwords()\n",
    "    all_words = [re.sub(r'[^\\w\\s]','', x) for x in words] # remove punctuation\n",
    "    all_words = [x for x in all_words if x not in stopwords]\n",
    "    return all_words\n",
    "\n",
    "# stemming\n",
    "# mengubah kata-kata menjadi kata dasarnya\n",
    "# menghilangkan imbuhan pada kata\n",
    "def stem_words(words):\n",
    "\tfactory = StemmerFactory()\n",
    "\tstemmer = factory.create_stemmer()\n",
    "\tall_words = [stemmer.stem(word) for word in words]\n",
    "\treturn all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['musik', 'sejuk', 'konser', 'sahabat', 'tidak', 'olahraga', 'jengkel', 'letih', 'pandang', 'musuh', 'bahagia', 'sungguh', 'mobil', 'dengar', 'makan', 'senang', 'cinta', 'cerah', 'tarik', 'indah', 'asa', 'bagus', 'suka']) 23\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(remove_stopwords(stem_words(get_words(data)))))\n",
    "\n",
    "word_features = get_word_features(all_words)\n",
    "# most_common = [(key, val) for key, val\n",
    "#                in nltk.FreqDist(remove_stopwords(stem_words(get_words(data)))).most_common()\n",
    "#                if val > 1]\n",
    "# print(most_common)\n",
    "print(word_features, len(word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mendapatkan fitur kata dari tiap dokumen\n",
    "# menjadikan array one-hot\n",
    "# dictionary {word: boolean presence}\n",
    "def extract_features(document):\n",
    "    document_words = sorted(set(remove_stopwords(stem_words(document))))\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'musik': False, 'sejuk': False, 'konser': False, 'sahabat': False, 'tidak': False, 'olahraga': False, 'jengkel': False, 'letih': False, 'senang': False, 'musuh': False, 'bahagia': False, 'sungguh': True, 'mobil': False, 'dengar': False, 'makan': False, 'pandang': True, 'cinta': False, 'cerah': False, 'tarik': False, 'indah': True, 'asa': False, 'bagus': False, 'suka': True}, 'positive')\n"
     ]
    }
   ],
   "source": [
    "# loading training data sets\n",
    "training_set = [(extract_features(kalimat), label) for kalimat, label in data]\n",
    "print(training_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'musik': False, 'sejuk': False, 'konser': False, 'sahabat': False, 'tidak': False, 'olahraga': False, 'jengkel': False, 'letih': False, 'senang': False, 'musuh': False, 'bahagia': False, 'sungguh': False, 'mobil': True, 'dengar': False, 'makan': False, 'pandang': False, 'cinta': False, 'cerah': False, 'tarik': False, 'indah': False, 'asa': False, 'bagus': False, 'suka': True}\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "n data awal: 17\n",
      "n data hasil: 22\n",
      "\n",
      "[(0, 'musik'), (1, 'sejuk'), (2, 'konser'), (3, 'sahabat'), (4, 'tidak'), (5, 'olahraga'), (6, 'jengkel'), (7, 'letih'), (8, 'senang'), (9, 'musuh'), (10, 'bahagia'), (11, 'sungguh'), (12, 'mobil'), (13, 'dengar'), (14, 'makan'), (15, 'pandang'), (16, 'cinta'), (17, 'cerah'), (18, 'tarik'), (19, 'indah'), (20, 'asa'), (21, 'bagus'), (22, 'suka')]\n",
      "\n",
      "1 [12, 22] ['mobil' 'suka'] 1\n",
      "2 [11, 15, 19, 22] ['sungguh' 'pandang' 'indah' 'suka'] 1\n",
      "3 [10] ['bahagia'] 1\n",
      "4 [2, 18] ['konser' 'tarik'] 1\n",
      "5 [3] ['sahabat'] 1\n",
      "6 [14, 22] ['makan' 'suka'] 1\n",
      "7 [16] ['cinta'] 1\n",
      "8 [10] ['bahagia'] 1\n",
      "9 [5, 8] ['olahraga' 'senang'] 1\n",
      "10 [1, 17, 20] ['sejuk' 'cerah' 'asa'] 1\n",
      "11 [0, 10, 13] ['musik' 'bahagia' 'dengar'] 1\n",
      "12 [4, 12, 22] ['tidak' 'mobil' 'suka'] 0\n",
      "13 [4, 11, 15, 21] ['tidak' 'sungguh' 'pandang' 'bagus'] 0\n",
      "14 [6, 11] ['jengkel' 'sungguh'] 0\n",
      "15 [7] ['letih'] 0\n",
      "16 [2, 4, 18] ['konser' 'tidak' 'tarik'] 0\n",
      "17 [9] ['musuh'] 0\n",
      "18 [7] ['letih'] 0\n",
      "19 [4, 11, 15, 21] ['tidak' 'sungguh' 'pandang' 'bagus'] 0\n",
      "20 [6, 11] ['jengkel' 'sungguh'] 0\n",
      "21 [7] ['letih'] 0\n",
      "22 [7] ['letih'] 0\n"
     ]
    }
   ],
   "source": [
    "# khusus untuk penanganan imbalanced data\n",
    "# \n",
    "# \n",
    "\n",
    "# mengubah fitur kata menjadi array bit (0 dan 1)\n",
    "featuresets = [extract_features(kalimat) for kalimat in X]\n",
    "featuresets_bit = []\n",
    "temp = []\n",
    "for index, i in enumerate(featuresets):\n",
    "    for j, k in i.items():\n",
    "        temp.append(1 if k == True else 0)\n",
    "    featuresets_bit.append(temp)\n",
    "    temp = []\n",
    "\n",
    "print(featuresets[0])\n",
    "print(featuresets_bit[0], end='\\n\\n')\n",
    "\n",
    "# oversampling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "y_bit = [1 if i == 'positive' else 0 for i in y]\n",
    "X_resampled, y_resampled = SMOTE().fit_sample(featuresets_bit, y_bit)\n",
    "\n",
    "# mengubah bilangan real menjadi 0 atau 1 dengan pembulatan\n",
    "X_resampled_normalized = []\n",
    "temp = []\n",
    "for i in X_resampled:\n",
    "    for j in i:\n",
    "        temp.append(round(j))\n",
    "    X_resampled_normalized.append(temp)\n",
    "    temp = []\n",
    "    \n",
    "print('n data awal:', len(X))\n",
    "print('n data hasil:', len(X_resampled_normalized), end='\\n\\n')\n",
    "\n",
    "# coba mengembalikan dari vector ke kata\n",
    "kamus = [(index, val) for index, val in enumerate(featuresets[0])]\n",
    "print(kamus, end='\\n\\n')\n",
    "kamus = np.array([val for index, val in enumerate(featuresets[0])])\n",
    "for index, word in enumerate(X_resampled_normalized):\n",
    "    indices = [index for index, value in enumerate(word) if value == 1]\n",
    "    print(index+1, indices, np.take(kamus, indices), y_resampled[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'musik': False, 'sejuk': False, 'konser': False, 'sahabat': False, 'tidak': False, 'olahraga': False, 'jengkel': False, 'letih': False, 'senang': True, 'musuh': False, 'bahagia': False, 'sungguh': False, 'mobil': False, 'dengar': False, 'makan': False, 'pandang': False, 'cinta': False, 'cerah': False, 'tarik': False, 'indah': False, 'asa': False, 'bagus': False, 'suka': False}\n",
      "positive\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "# dengan menggunakan algoritma naive bayes classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# print(classifier.show_most_informative_features())\n",
    "\n",
    "# validasi dengan menggunakan kalimat sendiri\n",
    "kalimat_tes = 'hAri Ini mEnyenAnGkan'\n",
    "kalimat_tes = extract_features(kalimat_tes.split())\n",
    "print(kalimat_tes)\n",
    "\n",
    "print(classifier.classify(kalimat_tes))\n",
    "\n",
    "# membandkan dengan SVM\n",
    "LinearSVC_clf = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_clf.train(training_set)\n",
    "print(LinearSVC_clf.classify(kalimat_tes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
