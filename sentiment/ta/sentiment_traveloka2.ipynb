{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import pickle, random, time, json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from preprocess import get_normalized_data\n",
    "\n",
    "from bow import make_vocabs\n",
    "\n",
    "from vectorizers import binary_vectorizer, \\\n",
    "    count_vectorizer, \\\n",
    "    tfidf_vectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, \\\n",
    "    GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from model_utils import get_best_model, \\\n",
    "    do_training_testing, \\\n",
    "    do_validation\n",
    "\n",
    "from smote import SMOTE as smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the file (Traveloka hotel comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n pos: 1058, n neg: 442, total: 1500\n",
      "sampel data positif: ['suka dekat bukit bintang transportasi gol murah lagi']\n",
      "sampel data negatif: ['kurang cocok jalan bisnis rombong privasi praktis privilese urus bisnis kamar tidak pandu arah sholat muslim padahal negara muslim front desak sifat kurang ramah banyak tidak lokasi tuju umum kol']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Membaca data komentar hotel traveloka\n",
    "berupa file json\n",
    "file json diambil 1500 komentar yang sudah diberi tag label secara manual\n",
    "bentuk format dari file json: [{'class': , 'text': }]\n",
    "\"\"\"\n",
    "with open('data.json', 'r') as file:\n",
    "    raw_data = json.load(file)\n",
    "\n",
    "n_data = 1500\n",
    "raw_data = raw_data[:n_data]\n",
    "random.seed(123)\n",
    "random.shuffle(raw_data)\n",
    "\n",
    "pos_texts_normalized, neg_texts_normalized = get_normalized_data(raw_data)\n",
    "\n",
    "n_neg = len(neg_texts_normalized)\n",
    "n_pos = n_data - n_neg\n",
    "\n",
    "print('n pos: {}, n neg: {}, total: {}'.format(n_pos, n_neg, n_data))\n",
    "print('sampel data positif:', random.sample(pos_texts_normalized, 1))\n",
    "print('sampel data negatif:', random.sample(neg_texts_normalized, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data -> training/testing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data training/testing:\t900 pos : 300 neg\n",
      "data validasi:\t\t158 pos : 142 neg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pemisahan data menjadi data training/testing dan data validasi\n",
    "\n",
    "Untuk data validasi diambil 300 data\n",
    "Data trainig/testing adalah seluruh data setelah dipotong untuk data validasi\n",
    "\n",
    "masing-masing disimpan setelah dikelompokkan berdasarkan kelasnya\n",
    "\"\"\"\n",
    "n_val_pos = 158\n",
    "n_val_neg = 142\n",
    "\n",
    "pos_tt = pos_texts_normalized[:-n_val_pos]\n",
    "neg_tt = neg_texts_normalized[:-n_val_neg]\n",
    "\n",
    "pos_val = pos_texts_normalized[-n_val_pos:]\n",
    "neg_val = neg_texts_normalized[-n_val_neg:]\n",
    "\n",
    "print('data training/testing:\\t{} pos : {} neg'.format(len(pos_tt), len(neg_tt)))\n",
    "print('data validasi:\\t\\t{} pos : {} neg'.format(len(pos_val), len(neg_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n fitur awal:\t\t 1739\n",
      "used n feature:\t\t 953\n"
     ]
    }
   ],
   "source": [
    "all_words = make_vocabs(pos_tt + neg_tt)\n",
    "\n",
    "print('used n feature:\\t\\t', len(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize features (binary, tf, tfidf)\n",
    "and do resampling, using SMOTE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_fitur = ['biner', 'count', 'tfidf']\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\"\"\"\n",
    "instansiasi object vectorizer\n",
    "untuk masing-masing fitur: biner, frekuensi, dan tf-idf\n",
    "kemudian dibentuk dict vectorizers\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizers = dict(zip(array_fitur, [\n",
    "    binary_vectorizer,\n",
    "    count_vectorizer,\n",
    "    TfidfVectorizer(vocabulary=all_words),\n",
    "#     tfidf_vectorizer,\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "vektorisasi data sebelum di-oversampling\n",
    "\n",
    "X = array of dict {fitur: data} yang akan digunakan untuk proses training dan testing\n",
    "y = label data untuk proses training dan testing\n",
    "\"\"\"\n",
    "# X = {fitur: vectorizers[fitur](pos_tt + neg_tt, all_words) for fitur in array_fitur}\n",
    "X = {}\n",
    "for fitur in array_fitur:\n",
    "    if fitur == 'tfidf':\n",
    "        vectorizers[fitur].fit(pos_tt + neg_tt)\n",
    "        X[fitur] = vectorizers[fitur].transform(pos_tt + neg_tt).toarray()\n",
    "#          X[fitur] = vectorizers[fitur](pos_tt + neg_tt, all_words, training=True)\n",
    "    else:\n",
    "        X[fitur] = vectorizers[fitur](pos_tt + neg_tt, all_words)\n",
    "y = np.concatenate([np.ones(len(pos_tt)), np.zeros(len(neg_tt))])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "oversampling data yang sudah divektorisasi\n",
    "\n",
    "data_resampled = data hasil oversampling dari X dan y\n",
    "X_resampled = array of dict {fitur: data} yang akan digunakan untuk proses training dan testing\n",
    "y_resampled = label data untuk proses training dan testing\n",
    "\"\"\"\n",
    "data_resampled = {fitur: smote(X[fitur], y, 200, k=3, random_seed=10) for fitur in array_fitur}\n",
    "X_resampled = {fitur: data_resampled[fitur][0] for fitur in array_fitur}\n",
    "y_resampled = data_resampled[array_fitur[0]][1]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "vektorisasi data murni untuk validasi, tanpa pengaruh oversampling\n",
    "\n",
    "X_val = array of dict {fitur: data} yang akan digunakan untuk proses validasi\n",
    "y_val = label data untuk proses validasi\n",
    "\"\"\"\n",
    "# X_val = {fitur: vectorizers[fitur].fit_transform(pos_val + neg_val).toarray() for fitur in array_fitur}\n",
    "X_val = {}\n",
    "for fitur in array_fitur:\n",
    "    if fitur == 'tfidf':\n",
    "        X_val[fitur] = vectorizers[fitur].transform(pos_val + neg_val).toarray()\n",
    "#         X_val[fitur] = vectorizers[fitur](pos_val + neg_val, all_words)\n",
    "    else:\n",
    "        X_val[fitur] = vectorizers[fitur](pos_val + neg_val, all_words)\n",
    "y_val = np.concatenate([np.ones(len(pos_val)), np.zeros(len(neg_val))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = OrderedDict(sorted(vectorizers.items()))\n",
    "X = OrderedDict(sorted(X.items()))\n",
    "X_resampled =  OrderedDict(sorted(X_resampled.items()))\n",
    "X_val =  OrderedDict(sorted(X_val.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information about the data portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data asli \t\t--> 1200 data \t\t--> pos : neg = (900, 300)\n",
      "data resampling \t--> 1800 data \t\t--> pos : neg = (900, 900)\n",
      "data validasi \t\t--> 300 data \t\t--> pos : neg = (158, 142)\n"
     ]
    }
   ],
   "source": [
    "def get_porsi(y):\n",
    "    n_pos = len([n for n in y if n == 1])\n",
    "    n_neg = len([n for n in y if n == 0])\n",
    "    return n_pos, n_neg\n",
    "\n",
    "print('data asli \\t\\t--> {} data \\t\\t--> pos : neg = {}'\n",
    "      .format(len(y), get_porsi(y)))\n",
    "print('data resampling \\t--> {} data \\t\\t--> pos : neg = {}'\n",
    "      .format(len(y_resampled), get_porsi(y_resampled)))\n",
    "print('data validasi \\t\\t--> {} data \\t\\t--> pos : neg = {}'\n",
    "      .format(len(y_val), get_porsi(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "array object dari model-model classifier yang akan digunakan\n",
    "menjadi paramater untuk beberapa fungsi\n",
    "\"\"\"\n",
    "\n",
    "clf = {\n",
    "    'multi_nb': MultinomialNB(),\n",
    "    'gauss_nb': GaussianNB(),\n",
    "    'svm_lin': LinearSVC(random_state=123),\n",
    "    'log_reg': LogisticRegression(random_state=123),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do training-testing and validation\n",
    "print the performance result for each algorithms and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============VALIDASI===============\n",
      "\n",
      "NO SAMPLING\n",
      "multi_nb biner: \t\t 82.17\n",
      "multi_nb count: \t\t 83.26\n",
      "gauss_nb tfidf: \t\t 59.41\n",
      ">> performa rata2 multi_nb: \t(74.95)\n",
      "svm_lin biner: \t\t\t 76.7\n",
      "svm_lin count: \t\t\t 79.38\n",
      "svm_lin tfidf: \t\t\t 78.43\n",
      ">> performa rata2 svm_lin: \t(78.17)\n",
      "log_reg biner: \t\t\t 76.88\n",
      "log_reg count: \t\t\t 79.99\n",
      "log_reg tfidf: \t\t\t 72.46\n",
      ">> performa rata2 log_reg: \t(76.44)\n",
      "selesai dalam 1.92 detik -> 0.03 menit\n",
      "\n",
      "RESAMPLED\n",
      "multi_nb biner: \t\t 87.7\n",
      "multi_nb count: \t\t 88.85\n",
      "gauss_nb tfidf: \t\t 60.9\n",
      ">> performa rata2 multi_nb: \t(79.15)\n",
      "svm_lin biner: \t\t\t 82.71\n",
      "svm_lin count: \t\t\t 85.47\n",
      "svm_lin tfidf: \t\t\t 86.63\n",
      ">> performa rata2 svm_lin: \t(84.94)\n",
      "log_reg biner: \t\t\t 86.29\n",
      "log_reg count: \t\t\t 86.29\n",
      "log_reg tfidf: \t\t\t 88.0\n",
      ">> performa rata2 log_reg: \t(86.86)\n",
      "selesai dalam 3.36 detik -> 0.06 menit\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "__main__\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print('===============VALIDASI===============\\n')\n",
    "\n",
    "start = time.time()\n",
    "print('NO SAMPLING')\n",
    "best_models_no_sampling = do_training_testing(\n",
    "    clf, X, y,\n",
    "    filename='kinerja_training_no_sampling.csv', show=False)\n",
    "do_validation(\n",
    "    clf, X_val, y_val, per_clf=best_models_no_sampling,\n",
    "    filename = 'kinerja_testing_no_sampling.csv')\n",
    "end = time.time()\n",
    "print('selesai dalam {} detik -> {} menit'.format(round(end-start, 2),\n",
    "                                                  round((end-start)/60, 2)))\n",
    "\n",
    "start = time.time()\n",
    "print('\\nRESAMPLED')\n",
    "best_models_after_sampling = do_training_testing(\n",
    "    clf, X_resampled, y_resampled,\n",
    "    filename='kinerja_training_with_sampling.csv', show=False)\n",
    "do_validation(\n",
    "    clf, X_val, y_val, per_clf=best_models_after_sampling,\n",
    "    filename = 'kinerja_testing_with_sampling.csv')\n",
    "end = time.time()\n",
    "print('selesai dalam {} detik -> {} menit'.format(round(end-start, 2),\n",
    "                                                  round((end-start)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time 51.21 detik -> 0.85 menit\n"
     ]
    }
   ],
   "source": [
    "END = time.time()\n",
    "print('total time {} detik -> {} menit'.format(round(END-START, 2),\n",
    "                                               round((END-START)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
