{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from stopwords import get_stopwords\n",
    "import corpus\n",
    "from collections import Counter\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n pos: 11\n",
      "n neg: 6\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "pos_data = corpus.pos\n",
    "neg_data = corpus.neg\n",
    "print('n pos:' , len(pos_data))\n",
    "print('n neg:', len(neg_data))\n",
    "\n",
    "# gabung semua data (pos dan neg)\n",
    "data = [] # teks dan label\n",
    "X = [] # teks saja\n",
    "y = [] # label/sentiment saja\n",
    "for (words, sentiment) in pos_data + neg_data:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3] \n",
    "    data.append((words_filtered, sentiment))\n",
    "    X.append(words_filtered)\n",
    "    y.append(sentiment)\n",
    "    \n",
    "index_pos = [(index) for index, value in enumerate(y) if value == 'positive']\n",
    "index_neg = [(index) for index, value in enumerate(y) if value == 'negative']\n",
    "\n",
    "pos_data = [X[index] for index in index_pos]\n",
    "pos_data_y = [y[index] for index in index_pos]\n",
    "neg_data = [X[index] for index in index_neg]\n",
    "neg_data_y = [y[index] for index in index_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenisasi\n",
    "# mentransformasi seluruh kalimat dalam corpus menjadi array dari\n",
    "# kata-kata\n",
    "def get_words(data):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in data:\n",
    "    \tall_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "# mendapatkan word features\n",
    "# atau kamus dari seluruh korpus\n",
    "# mengurutkan kata-kata dari frekuensi kemunculan tertinggi\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "# hapus stopwords dan tanda baca\n",
    "def remove_stopwords(words):\n",
    "    stopwords = get_stopwords()\n",
    "    all_words = [re.sub(r'[^\\w\\s]','', x) for x in words] # remove punctuation\n",
    "    all_words = [x for x in all_words if x not in stopwords]\n",
    "    return all_words\n",
    "\n",
    "# stemming\n",
    "# mengubah kata-kata menjadi kata dasarnya\n",
    "# menghilangkan imbuhan pada kata\n",
    "def stem_words(words):\n",
    "\tfactory = StemmerFactory()\n",
    "\tstemmer = factory.create_stemmer()\n",
    "\tall_words = [stemmer.stem(word) for word in words]\n",
    "\treturn all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['indah', 'mobil', 'letih', 'sungguh', 'olahraga', 'cinta', 'konser', 'bahagia', 'pandang', 'bagus', 'jengkel', 'sahabat', 'senang', 'musik', 'cerah', 'sejuk', 'suka', 'asa', 'tarik', 'dengar', 'tidak', 'makan', 'musuh']) 23\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(remove_stopwords(stem_words(get_words(data)))))\n",
    "\n",
    "word_features = get_word_features(all_words)\n",
    "# most_common = [(key, val) for key, val\n",
    "#                in nltk.FreqDist(remove_stopwords(stem_words(get_words(data)))).most_common()\n",
    "#                if val > 1]\n",
    "# print(most_common)\n",
    "print(word_features, len(word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mendapatkan fitur kata dari tiap dokumen\n",
    "# menjadikan array one-hot\n",
    "# dictionary {word: boolean presence}\n",
    "def extract_features(document):\n",
    "    document_words = sorted(set(remove_stopwords(stem_words(document))))\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'indah': True, 'mobil': False, 'letih': False, 'sungguh': True, 'olahraga': False, 'pandang': True, 'konser': False, 'bahagia': False, 'cinta': False, 'bagus': False, 'jengkel': False, 'sahabat': False, 'senang': False, 'musik': False, 'cerah': False, 'sejuk': False, 'suka': True, 'asa': False, 'tarik': False, 'dengar': False, 'tidak': False, 'makan': False, 'musuh': False}, 'positive')\n"
     ]
    }
   ],
   "source": [
    "# loading training data sets\n",
    "training_set = [(extract_features(kalimat), label) for kalimat, label in data]\n",
    "print(training_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indah': False, 'mobil': True, 'letih': False, 'sungguh': False, 'olahraga': False, 'pandang': False, 'konser': False, 'bahagia': False, 'cinta': False, 'bagus': False, 'jengkel': False, 'sahabat': False, 'senang': False, 'musik': False, 'cerah': False, 'sejuk': False, 'suka': True, 'asa': False, 'tarik': False, 'dengar': False, 'tidak': False, 'makan': False, 'musuh': False}\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "n data awal: 17\n",
      "n data hasil: 22\n",
      "\n",
      "[(0, 'indah'), (1, 'mobil'), (2, 'letih'), (3, 'sungguh'), (4, 'olahraga'), (5, 'pandang'), (6, 'konser'), (7, 'bahagia'), (8, 'cinta'), (9, 'bagus'), (10, 'jengkel'), (11, 'sahabat'), (12, 'senang'), (13, 'musik'), (14, 'cerah'), (15, 'sejuk'), (16, 'suka'), (17, 'asa'), (18, 'tarik'), (19, 'dengar'), (20, 'tidak'), (21, 'makan'), (22, 'musuh')]\n",
      "\n",
      "1 [1, 16] ['mobil' 'suka'] 1\n",
      "2 [0, 3, 5, 16] ['indah' 'sungguh' 'pandang' 'suka'] 1\n",
      "3 [7] ['bahagia'] 1\n",
      "4 [6, 18] ['konser' 'tarik'] 1\n",
      "5 [11] ['sahabat'] 1\n",
      "6 [16, 21] ['suka' 'makan'] 1\n",
      "7 [8] ['cinta'] 1\n",
      "8 [7] ['bahagia'] 1\n",
      "9 [4, 12] ['olahraga' 'senang'] 1\n",
      "10 [14, 15, 17] ['cerah' 'sejuk' 'asa'] 1\n",
      "11 [7, 13, 19] ['bahagia' 'musik' 'dengar'] 1\n",
      "12 [1, 16, 20] ['mobil' 'suka' 'tidak'] 0\n",
      "13 [3, 5, 9, 20] ['sungguh' 'pandang' 'bagus' 'tidak'] 0\n",
      "14 [3, 10] ['sungguh' 'jengkel'] 0\n",
      "15 [2] ['letih'] 0\n",
      "16 [6, 18, 20] ['konser' 'tarik' 'tidak'] 0\n",
      "17 [22] ['musuh'] 0\n",
      "18 [1, 16, 20] ['mobil' 'suka' 'tidak'] 0\n",
      "19 [3, 10] ['sungguh' 'jengkel'] 0\n",
      "20 [22] ['musuh'] 0\n",
      "21 [3, 10] ['sungguh' 'jengkel'] 0\n",
      "22 [1, 16, 20] ['mobil' 'suka' 'tidak'] 0\n"
     ]
    }
   ],
   "source": [
    "# khusus untuk penanganan imbalanced data\n",
    "# \n",
    "# \n",
    "\n",
    "# mengubah fitur kata menjadi array bit (0 dan 1)\n",
    "featuresets = [extract_features(kalimat) for kalimat in X]\n",
    "featuresets_bit = []\n",
    "temp = []\n",
    "for index, i in enumerate(featuresets):\n",
    "    for j, k in i.items():\n",
    "        temp.append(1 if k == True else 0)\n",
    "    featuresets_bit.append(temp)\n",
    "    temp = []\n",
    "\n",
    "print(featuresets[0])\n",
    "print(featuresets_bit[0], end='\\n\\n')\n",
    "\n",
    "# oversampling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "y_bit = [1 if i == 'positive' else 0 for i in y]\n",
    "X_resampled, y_resampled = SMOTE().fit_sample(featuresets_bit, y_bit)\n",
    "\n",
    "# mengubah bilangan real menjadi 0 atau 1 dengan pembulatan\n",
    "X_resampled_normalized = []\n",
    "temp = []\n",
    "for i in X_resampled:\n",
    "    for j in i:\n",
    "        temp.append(round(j))\n",
    "    X_resampled_normalized.append(temp)\n",
    "    temp = []\n",
    "    \n",
    "print('n data awal:', len(X))\n",
    "print('n data hasil:', len(X_resampled_normalized), end='\\n\\n')\n",
    "\n",
    "# coba mengembalikan dari vector ke kata\n",
    "kamus = [(index, val) for index, val in enumerate(featuresets[0])]\n",
    "print(kamus, end='\\n\\n')\n",
    "kamus = np.array([val for index, val in enumerate(featuresets[0])])\n",
    "for index, word in enumerate(X_resampled_normalized):\n",
    "    indices = [index for index, value in enumerate(word) if value == 1]\n",
    "    print(index+1, indices, np.take(kamus, indices), y_resampled[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indah': False, 'mobil': False, 'letih': False, 'sungguh': False, 'olahraga': False, 'pandang': False, 'konser': False, 'bahagia': False, 'cinta': False, 'bagus': False, 'jengkel': False, 'sahabat': False, 'senang': True, 'musik': False, 'cerah': False, 'sejuk': False, 'suka': False, 'asa': False, 'tarik': False, 'dengar': False, 'tidak': False, 'makan': False, 'musuh': False}\n",
      "positive\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "# dengan menggunakan algoritma naive bayes classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "# print(classifier.show_most_informative_features())\n",
    "\n",
    "# validasi dengan menggunakan kalimat sendiri\n",
    "kalimat_tes = 'hAri Ini mEnyenAnGkan'\n",
    "kalimat_tes = extract_features(kalimat_tes.split())\n",
    "print(kalimat_tes)\n",
    "\n",
    "print(classifier.classify(kalimat_tes))\n",
    "\n",
    "# membandkan dengan SVM\n",
    "LinearSVC_clf = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_clf.train(training_set)\n",
    "print(LinearSVC_clf.classify(kalimat_tes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
