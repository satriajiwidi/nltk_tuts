{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os.path import isfile\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from stopwords import get_stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score as gmean\n",
    "from smote import MY_SMOTE as smote\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the file (Traveloka hotel comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n pos: 1058, n neg: 442, total: 1500\n",
      "sampel data positif: ['lokasi strategis kl sentral station monorail banyak restoran hotel kamar bersih besar bangun baru toiletries lengkap sedia safe deposit box kulkas hair dryer water heater']\n",
      "sampel data negatif: ['tidak air panas sudah telepon resepsionist tidak direspon']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Membaca data komentar hotel traveloka\n",
    "berupa file json\n",
    "file json diambil 1500 komentar yang sudah diberi tag label secara manual\n",
    "bentuk format dari file json: [{'class': , 'text': }]\n",
    "kemudian pisah kedalam dua variabel:\n",
    "    labels: array untuk label/kategori/kelas sentimen dari tiap komentar\n",
    "    texts: array untuk komentar\n",
    "\"\"\"\n",
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "data = data[:1500]\n",
    "labels = [d['class'] for d in data]\n",
    "texts = [d['text'] for d in data]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Preprocessing data\n",
    "1. pisahkan text positif dan negatif ke dalam variabel array masing-masing\n",
    "2. untuk masing-masing text positif maupun negatif, lakukan:\n",
    "    3. hapus stopwords\n",
    "    4. stem tiap kata\n",
    "5. simpan sebagai pickle file\n",
    "\"\"\"\n",
    "if isfile('data_traveloka_normalized.data'):\n",
    "    with open('data_traveloka_normalized.data', 'rb') as data:\n",
    "        pos_texts_normalized, neg_texts_normalized = pickle.load(data)\n",
    "else:\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    stopwords = get_stopwords()\n",
    "    \n",
    "    pos_texts = [d['text'] for d in data if d['class'] == 1]\n",
    "    neg_texts = [d['text'] for d in data if d['class'] == 0]\n",
    "\n",
    "    pos_texts_normalized = []\n",
    "    for text in pos_texts:\n",
    "        text = [stemmer.stem(word) for word in text.split() if word not in stopwords]\n",
    "        pos_texts_normalized.append(' '.join(text))\n",
    "        \n",
    "    neg_texts_normalized = []\n",
    "    for text in neg_texts:\n",
    "        text = [stemmer.stem(word) for word in text.split() if word not in stopwords]\n",
    "        neg_texts_normalized.append(' '.join(text))\n",
    "        \n",
    "    with open('data_traveloka_normalized.data', 'wb') as data:\n",
    "        pickle.dump([pos_texts_normalized, neg_texts_normalized], data)\n",
    "\n",
    "        \n",
    "n_pos, n_neg = len(pos_texts_normalized), len(neg_texts_normalized)\n",
    "n_total = n_pos + n_neg\n",
    "print('n pos: {}, n neg: {}, total: {}'.format(n_pos, n_neg, n_total))\n",
    "print('sampel data positif:', random.sample(pos_texts_normalized, 1))\n",
    "print('sampel data negatif:', random.sample(neg_texts_normalized, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data -> training/testing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data training/testing:\t958 pos : 342 neg\n",
      "data validasi:\t\t100 pos : 100 neg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pemisahan data menjadi data training/testing dan data validasi\n",
    "\n",
    "Untuk data validasi diambil masing-masing 100 data positif dan negatif\n",
    "Data trainig/testing adalah seluruh data setelah dipotong untuk data validasi\n",
    "\n",
    "masing-masing disimpan setelah dikelompokkan berdasarkan kelasnya\n",
    "\"\"\"\n",
    "n_val = 100\n",
    "\n",
    "pos_tt = pos_texts_normalized[:-n_val]\n",
    "neg_tt = neg_texts_normalized[:-n_val]\n",
    "\n",
    "pos_val = pos_texts_normalized[-n_val:]\n",
    "neg_val = neg_texts_normalized[-n_val:]\n",
    "\n",
    "\n",
    "print('data training/testing:\\t{} pos : {} neg'.format(len(pos_tt), len(neg_tt)))\n",
    "print('data validasi:\\t\\t{} pos : {} neg'.format(len(pos_val), len(neg_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n fitur awal:\t\t 2448\n",
      "used n feature:\t\t 2448\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Proses pembuatan vocabs\n",
    "\n",
    "vocabs ini digunakan untuk membentuk feature vector dari normalized data\n",
    "beberapa perlakukan untuk membentuk vocabs, di antarnya:\n",
    "(1) hapus hapax: kata yang hanya muncul sekali dari seluruh corpus\n",
    "(2) seleksi hanya kata kerja\n",
    "\"\"\"\n",
    "all_words = [word for sentence in pos_tt + neg_tt for word in sentence.split()]\n",
    "\n",
    "fd = FreqDist(all_words) # sebelum di-set, bentuk object freqdist\n",
    "\n",
    "all_words = list(set(all_words))\n",
    "print('n fitur awal:\\t\\t', len(all_words))\n",
    "\n",
    "# (1)\n",
    "hapaxes = fd.hapaxes()\n",
    "# all_words = [word for word in all_words if word not in hapaxes]\n",
    "\n",
    "# (2)\n",
    "with open('./experiment/pos_tag_indo.pkl', 'rb') as file:\n",
    "    jj = pickle.load(file)\n",
    "all_words_adj = [word for word in all_words if word in jj]\n",
    "# all_words = all_words_adj\n",
    "\n",
    "print('used n feature:\\t\\t', len(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized features (binary, tf, tfidf)\n",
    "and do resampling, using SMOTE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_fitur = ['biner', 'tfrek', 'tfidf']\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "instansiasi object vectorizer\n",
    "untuk masing-masing fitur: biner, frekuensi, dan tf-idf\n",
    "kemudian dibentuk dict vectorizers\n",
    "\"\"\"\n",
    "vectorizers = dict(zip(array_fitur, [\n",
    "    CountVectorizer(binary=True, vocabulary=all_words),\n",
    "    CountVectorizer(binary=False, vocabulary=all_words),\n",
    "    TfidfVectorizer(vocabulary=all_words)\n",
    "]))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "vektorisasi data sebelum di-oversampling\n",
    "\n",
    "X = array of dict {fitur: data} yang akan digunakan untuk proses training dan testing\n",
    "y = label data untuk proses training dan testing\n",
    "\"\"\"\n",
    "X = {fitur: vectorizers[fitur].fit_transform(pos_tt + neg_tt).toarray() for fitur in array_fitur}\n",
    "y = np.concatenate([np.ones(len(pos_tt)), np.zeros(len(neg_tt))])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "oversampling data yang sudah divektorisasi\n",
    "\n",
    "data_resampled = data hasil oversampling dari X dan y\n",
    "X_resampled = array of dict {fitur: data} yang akan digunakan untuk proses training dan testing\n",
    "y_resampled = label data untuk proses training dan testing\n",
    "\"\"\"\n",
    "data_resampled = {fitur: smote(X[fitur], y, 100, k=3, random_seed=10) for fitur in array_fitur}\n",
    "\n",
    "X_resampled = {fitur: data_resampled[fitur][0] for fitur in array_fitur}\n",
    "y_resampled = {fitur: data_resampled[fitur][1] for fitur in array_fitur}\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "vektorisasi data murni untuk validasi, tanpa pengaruh oversampling\n",
    "\n",
    "X_val = array of dict {fitur: data} yang akan digunakan untuk proses validasi\n",
    "y_val = label data untuk proses validasi\n",
    "\"\"\"\n",
    "X_val = {fitur: vectorizers[fitur].fit_transform(pos_val + neg_val).toarray() for fitur in array_fitur}\n",
    "y_val = np.concatenate([np.ones(len(pos_val)), np.zeros(len(neg_val))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information about the data portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data asli \t\t--> 1300 data \t\t--> pos : neg = (958, 342)\n",
      "data resampling \t--> 1642 data \t\t--> pos : neg = (958, 684)\n",
      "data validasi \t\t--> 200 data \t\t--> pos : neg = (100, 100)\n"
     ]
    }
   ],
   "source": [
    "def get_porsi(y):\n",
    "    n_pos = len([n for n in y if n == 1])\n",
    "    n_neg = len([n for n in y if n == 0])\n",
    "    return n_pos, n_neg\n",
    "\n",
    "print('data asli \\t\\t--> {} data \\t\\t--> pos : neg = {}'\n",
    "      .format(len(y), get_porsi(y)))\n",
    "print('data resampling \\t--> {} data \\t\\t--> pos : neg = {}'\n",
    "      .format(len(y_resampled['biner']), get_porsi(y_resampled['biner'])))\n",
    "print('data validasi \\t\\t--> {} data \\t\\t--> pos : neg = {}'\n",
    "      .format(len(y_val), get_porsi(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(X, y, c, kf, show=False):\n",
    "    \"\"\"\n",
    "    fungsi untuk mendapatkan model terbaik dari hasil k-fold\n",
    "    return best_model: model terbaik, dengan tolak ukur gmean\n",
    "    \n",
    "    parameter:\n",
    "    X = data per jenis fitur\n",
    "    y = label dari data\n",
    "    c = array object classifier\n",
    "    kf = object K-Fold\n",
    "    show = boolean untuk mencetak proses pencarian model terbaik\n",
    "    \"\"\"\n",
    "    performance_total = 0\n",
    "    best_fold_performance = -100\n",
    "    best_fold_index = -1\n",
    "    best_model = None\n",
    "    if show: print('\\t\\t', end='')\n",
    "    for index, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "        y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "        c.fit(X_train_fold, y_train_fold)\n",
    "        pred = c.predict(X_test_fold)\n",
    "        acc = round(accuracy_score(y_test_fold, pred) * 100, 2)\n",
    "        gmean_score = round(gmean(y_test_fold, pred, average='binary') * 100, 2)\n",
    "        selected_metric = gmean_score\n",
    "        if selected_metric > best_fold_performance:\n",
    "            best_fold_performance = selected_metric\n",
    "            best_fold_index = index\n",
    "            best_model = c\n",
    "        if show: print(selected_metric, end=' ')\n",
    "        performance_total += selected_metric\n",
    "    performance_avg = round(performance_total / kf.get_n_splits(), 2)\n",
    "    if show: print('\\n\\t\\tbest index: {}, best performance: {}, performance avg: {}\\n'\n",
    "                      .format(best_fold_index+1, best_fold_performance, performance_avg))\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def do_training_testing(clf, X, y, is_res=False, show=False):\n",
    "    \"\"\"\n",
    "    fungsi untuk melakukan training dan testing\n",
    "    baik itu dengan atau tanpa resampling\n",
    "    return per_clf: model terbaik dari masing-masing fitur\n",
    "    \n",
    "    parameter:\n",
    "    clf = array object classifier\n",
    "    X = data per jenis fitur\n",
    "    y = label dari data\n",
    "    is_res = boolean, untuk mengecek apakah data yg dilewatkan adalah hasil resampling atau tidak\n",
    "    show = boolean, untuk mencetak proses pencarian model terbaik\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    per_clf = {}\n",
    "    for c in clf: # untuk masing-masing jenis classifier\n",
    "        \n",
    "        for index, fitur in enumerate(X): # untuk masing-masing jenis fitur\n",
    "            y_train = y[fitur] if is_res else y\n",
    "            if show: # show process\n",
    "                print('\\t', c, fitur)\n",
    "                per_clf[(c, fitur)] = get_best_model(X[fitur], y_train, clf[c], kf, show=True)\n",
    "            else:\n",
    "                per_clf[(c, fitur)] = get_best_model(X[fitur], y_train, clf[c], kf)\n",
    "    return per_clf\n",
    "\n",
    "\n",
    "\n",
    "def do_validation(clf, X_val, y_val, per_clf):\n",
    "    \"\"\"\n",
    "    fungsi untuk melakukan validasi\n",
    "    baik itu dengan atau tanpa resampling\n",
    "    \n",
    "    parameter:\n",
    "    clf = array object classifier\n",
    "    X_val = data per jenis fitur khusus untuk proses validasi\n",
    "    y_val = label dari data khusus untuk proses validasi\n",
    "    per_clf = model terbaik untuk masing-masing fitur\n",
    "    \"\"\"\n",
    "    for c in clf:\n",
    "        performance_total = 0\n",
    "        n_fitur = 0\n",
    "        for index, fitur in enumerate(X_val):\n",
    "            n_fitur += 1\n",
    "            pred = per_clf[c, fitur].predict(X_val[fitur])\n",
    "            \n",
    "            # performa menggunakan akurasi atau gmean\n",
    "            \n",
    "            gmean_score = round(gmean(y_val, pred, average='binary') * 100, 2)\n",
    "            acc = round(accuracy_score(y_val, pred) * 100, 2)\n",
    "            print(confusion_matrix(y_val, pred, labels=[1,0]))\n",
    "            performance_total += gmean_score\n",
    "            # print('acc:gm {} {}: \\t\\t {} : {}'.format(c, fitur, acc, gmean_score))\n",
    "            print('gm {} {}: \\t\\t {}'.format(c, fitur, gmean_score))\n",
    "        performance_avg = round(performance_total / n_fitur, 2)\n",
    "        print('>> performa rata2 {}: \\t({})'.format(c, performance_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "array object dari model-model classifier yang akan digunakan\n",
    "menjadi paramater untuk beberapa fungsi\n",
    "\"\"\"\n",
    "clf = {\n",
    "    'multi_nb': MultinomialNB(),\n",
    "    'supp_vm': SVC(),\n",
    "    'log_reg': LogisticRegression(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do training-testing and validation\n",
    "print the performance result for each algorithms and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============VALIDASI===============\n",
      "\n",
      "NO SAMPLING\n",
      "[[100   0]\n",
      " [100   0]]\n",
      "gm supp_vm tfidf: \t\t 0.0\n",
      "[[100   0]\n",
      " [100   0]]\n",
      "gm supp_vm tfrek: \t\t 0.0\n",
      "[[100   0]\n",
      " [100   0]]\n",
      "gm supp_vm biner: \t\t 0.0\n",
      ">> performa rata2 supp_vm: \t(0.0)\n",
      "[[98  2]\n",
      " [61 39]]\n",
      "gm multi_nb tfidf: \t\t 61.82\n",
      "[[97  3]\n",
      " [48 52]]\n",
      "gm multi_nb tfrek: \t\t 71.02\n",
      "[[97  3]\n",
      " [50 50]]\n",
      "gm multi_nb biner: \t\t 69.64\n",
      ">> performa rata2 multi_nb: \t(67.49)\n",
      "[[100   0]\n",
      " [ 74  26]]\n",
      "gm log_reg tfidf: \t\t 50.99\n",
      "[[96  4]\n",
      " [42 58]]\n",
      "gm log_reg tfrek: \t\t 74.62\n",
      "[[97  3]\n",
      " [45 55]]\n",
      "gm log_reg biner: \t\t 73.04\n",
      ">> performa rata2 log_reg: \t(66.22)\n",
      "selesai dalam 103.27 detik -> 1.72 menit\n",
      "\n",
      "RESAMPLED\n",
      "[[100   0]\n",
      " [100   0]]\n",
      "gm supp_vm tfidf: \t\t 0.0\n",
      "[[100   0]\n",
      " [100   0]]\n",
      "gm supp_vm tfrek: \t\t 0.0\n",
      "[[100   0]\n",
      " [100   0]]\n",
      "gm supp_vm biner: \t\t 0.0\n",
      ">> performa rata2 supp_vm: \t(0.0)\n",
      "[[95  5]\n",
      " [35 65]]\n",
      "gm multi_nb tfidf: \t\t 78.58\n",
      "[[95  5]\n",
      " [33 67]]\n",
      "gm multi_nb tfrek: \t\t 79.78\n",
      "[[95  5]\n",
      " [32 68]]\n",
      "gm multi_nb biner: \t\t 80.37\n",
      ">> performa rata2 multi_nb: \t(79.58)\n",
      "[[95  5]\n",
      " [34 66]]\n",
      "gm log_reg tfidf: \t\t 79.18\n",
      "[[93  7]\n",
      " [32 68]]\n",
      "gm log_reg tfrek: \t\t 79.52\n",
      "[[94  6]\n",
      " [33 67]]\n",
      "gm log_reg biner: \t\t 79.36\n",
      ">> performa rata2 log_reg: \t(79.35)\n",
      "selesai dalam 219.07 detik -> 3.65 menit\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "__main__\n",
    "\"\"\"\n",
    "\n",
    "# path = '/home/satriajiwidi/Desktop/codes/python_codes/web/pyweb/models/pt/'\n",
    "\n",
    "# with open(path + 'val_data.pkl', 'wb') as data:\n",
    "#     pickle.dump([X_val, y_val], data)\n",
    "#     print('validation data pickled')\n",
    "\n",
    "\n",
    "best_models = {}\n",
    "print('===============VALIDASI===============\\n')\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "print('NO SAMPLING')\n",
    "best_models_no_sampling = do_training_testing(clf, X, y, show=False)\n",
    "do_validation(clf, X_val, y_val, per_clf=best_models_no_sampling)\n",
    "end = time.time()\n",
    "print('selesai dalam {} detik -> {} menit'.format(round(end-start, 2),\n",
    "                                                  round((end-start)/60, 2)))\n",
    "\n",
    "# with open(path + 'pt/model_ns.pkl', 'wb') as model:\n",
    "#     pickle.dump(best_models_no_sampling, model)\n",
    "#     print('model pickled')\n",
    "\n",
    "start = time.time()\n",
    "print('\\nRESAMPLED')\n",
    "best_models_after_sampling = do_training_testing(clf, X_resampled, y_resampled, is_res=True, show=False)\n",
    "do_validation(clf, X_val, y_val, per_clf=best_models_after_sampling)\n",
    "end = time.time()\n",
    "print('selesai dalam {} detik -> {} menit'.format(round(end-start, 2),\n",
    "                                                  round((end-start)/60, 2)))\n",
    "\n",
    "# with open(path + 'pt/model_as.pkl', 'wb') as model:\n",
    "#     pickle.dump(best_models_after_sampling, model)\n",
    "#     print('model pickled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
