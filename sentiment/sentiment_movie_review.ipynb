{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os.path import isfile\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score as gmean\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the file (imdb movie reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 1000\n",
      "neg: 1000\n",
      "total: 2000\n"
     ]
    }
   ],
   "source": [
    "major, minor = 1000, 200\n",
    "test_size = .2\n",
    "pos = []\n",
    "neg = []\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "if isfile('pos.data'):\n",
    "    with open('pos.data', 'rb') as posfile:\n",
    "        pos = pickle.load(posfile)\n",
    "else:\n",
    "    for fileid in movie_reviews.fileids('pos')[:major]:\n",
    "        temp = []\n",
    "        for words in movie_reviews.words(fileid):\n",
    "            if words not in stopwords.words('english') and words not in punctuation:\n",
    "                temp.append(stemmer.stem(words))\n",
    "        pos.append(' '.join(temp))\n",
    "    with open('pos.data', 'wb') as posfile:\n",
    "        pickle.dump(pos, posfile)\n",
    "\n",
    "# read full neg, then random choose 200 data\n",
    "if isfile('neg_full.data'):\n",
    "    with open('neg_full.data', 'rb') as negfile:\n",
    "        neg = pickle.load(negfile)\n",
    "else:\n",
    "    for fileid in movie_reviews.fileids('neg')[:minor]:\n",
    "        temp = []\n",
    "        for words in movie_reviews.words(fileid):\n",
    "            if words not in stopwords.words('english') and words not in punctuation:\n",
    "                temp.append(stemmer.stem(words))\n",
    "        neg.append(' '.join(temp))\n",
    "    with open('neg.data', 'wb') as negfile:\n",
    "        pickle.dump(neg, negfile)\n",
    "\n",
    "print('pos:', len(pos))\n",
    "print('neg:', len(neg))\n",
    "print('total:', len(pos + neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data -> training/testing and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data training/testing:\t900 pos : 100 neg\n",
      "data validasi:\t\t100 pos : 100 neg\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "neg = random.sample(neg, 200)\n",
    "n_val = 100\n",
    "\n",
    "pos_tt = pos[:-n_val]\n",
    "neg_tt = neg[:-n_val]\n",
    "\n",
    "# pembagian data training-testing dengan data validasi (tanpa resample/asli)\n",
    "pos_val = pos[-n_val:]\n",
    "neg_val = neg[-n_val:]\n",
    "\n",
    "print('data training/testing:\\t{} pos : {} neg'.format(len(pos_tt), len(neg_tt)))\n",
    "print('data validasi:\\t\\t{} pos : {} neg'.format(len(pos_val), len(neg_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n fitur awal:\t\t 20124\n",
      "n fitur tanpa hapax:\t 13330\n",
      "n feature selection:\t 2465\n",
      "used n feature:\t\t 13330\n"
     ]
    }
   ],
   "source": [
    "# pembuatan vocabs\n",
    "all_words = [word for i in pos_tt + neg_tt for word in i.split()]\n",
    "# hapus hapax\n",
    "fd = FreqDist(all_words)\n",
    "all_words = list(set(all_words))\n",
    "print('n fitur awal:\\t\\t', len(all_words))\n",
    "hapaxes = fd.hapaxes()\n",
    "all_words = [word for word in all_words if word not in hapaxes]\n",
    "print('n fitur tanpa hapax:\\t', len(all_words))\n",
    "# all_words = [key[0] for key in fd.most_common(10000)]\n",
    "# print('n fitur final:', len(all_words))\n",
    "\n",
    "# seleksi fitur yg digunakan\n",
    "tagged = nltk.pos_tag(all_words)\n",
    "selection = ['JJ', 'JJR', 'JJS']\n",
    "excepts = ['NN', 'NNP', 'NNPS', 'NNS']\n",
    "all_words2 = [word for (word, tag) in tagged if tag in selection]\n",
    "# all_words2 = [word for (word, tag) in tagged if tag not in excepts]\n",
    "print('n feature selection:\\t', len(all_words2))\n",
    "\n",
    "# full vocabs/selection feature toggle\n",
    "# uncomment to use selection feature\n",
    "# all_words = all_words2\n",
    "print('used n feature:\\t\\t', len(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized features (binary, tf, tfidf)\n",
    "and do resampling, using SMOTE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitur = ['biner', 'tf', 'tfidf']\n",
    "\n",
    "vectorizer_types = [CountVectorizer(binary=True, vocabulary=all_words),\n",
    "                    CountVectorizer(binary=False, vocabulary=all_words),\n",
    "                    TfidfVectorizer(vocabulary=all_words)\n",
    "                   ]\n",
    "\n",
    "vectorizer = dict(zip(fitur, vectorizer_types))\n",
    "\n",
    "X = {f: vectorizer[f].fit_transform(pos_tt + neg_tt).toarray() for f in fitur}\n",
    "y = np.concatenate([np.ones(len(pos_tt)), np.zeros(len(neg_tt))])\n",
    "\n",
    "X_val = {f: vectorizer[f].fit_transform(pos_val + neg_val).toarray() for f in fitur}\n",
    "y_val = np.concatenate([np.ones(len(pos_val)), np.zeros(len(neg_val))])\n",
    "\n",
    "data_resampled = {f: SMOTE(random_state=0, kind='borderline2').fit_sample(X[f], y) for f in fitur}\n",
    "# data_resampled = {f: ADASYN(random_state=0).fit_sample(X[f], y) for f in fitur}\n",
    "\n",
    "X_resampled = {f: data_resampled[f][0] for f in fitur}\n",
    "\n",
    "y_resampled = {f: data_resampled[f][1] for f in fitur}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information about the data portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data asli \t\t--> 1000 data \t\t--> pos : neg = (900, 100)\n",
      "data resampling \t--> 1800 data \t\t--> pos : neg = (900, 900)\n",
      "data validasi \t\t--> 200 data \t\t--> pos : neg = (100, 100)\n"
     ]
    }
   ],
   "source": [
    "def get_porsi(y):\n",
    "    pos = len([n for n in y if n == 1])\n",
    "    neg = len([n for n in y if n == 0])\n",
    "    return pos, neg\n",
    "\n",
    "# cetak informasi data\n",
    "print('data asli \\t\\t--> {} data \\t\\t--> pos : neg = {}'\n",
    "      .format(len(y), get_porsi(y)))\n",
    "print('data resampling \\t--> {} data \\t\\t--> pos : neg = {}'\n",
    "      .format(len(y_resampled['biner']), get_porsi(y_resampled['biner'])))\n",
    "print('data validasi \\t\\t--> {} data \\t\\t--> pos : neg = {}'\n",
    "      .format(len(y_val), get_porsi(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "def get_best_model(X, y, c, verbose=False):\n",
    "    acc_total = 0\n",
    "    best_fold_acc = -100\n",
    "    best_fold_index = -1\n",
    "    best_model = None\n",
    "    if verbose: print('\\t\\t', end='')\n",
    "    for index, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        XXX_train, XXX_test = X[train_index], X[test_index]\n",
    "        YYY_train, YYY_test = y[train_index], y[test_index]\n",
    "        c.fit(XXX_train, YYY_train)\n",
    "        pred = c.predict(XXX_test)\n",
    "        acc = round(accuracy_score(YYY_test, pred) * 100, 2)\n",
    "        if acc > best_fold_acc:\n",
    "            best_fold_acc = acc\n",
    "            best_fold_index = index\n",
    "            best_model = c\n",
    "        if verbose: print(acc, end=' ')\n",
    "        acc_total += acc\n",
    "    acc_avg = round(acc_total / kf.get_n_splits(), 2)\n",
    "    if verbose: print('\\n\\t\\tbest index: {}, best acc: {}, avg acc: {}\\n'\n",
    "                      .format(best_fold_index+1, best_fold_acc, acc_avg))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnr(fn, tp):\n",
    "    return fn / (fn + tp)\n",
    "\n",
    "def fpr(fp, tn):\n",
    "    return fp / (fp + tn)\n",
    "\n",
    "def make_subplots(X):\n",
    "    # X adalah fitur, len(X) = banyak fitur\n",
    "    _, axs = plt.subplots(1, len(X))\n",
    "    plt.setp(axs, xticks=[0,1], xticklabels=[1,0],\n",
    "             yticks=[0,1], yticklabels=[1,0])\n",
    "    axs = axs.ravel()\n",
    "    return axs\n",
    "\n",
    "def show_cm(y, pred, axs, index, fitur, c, tp):\n",
    "    acc = round(accuracy_score(y, pred) * 100, 2)\n",
    "    cm = confusion_matrix(y, pred, labels=[1,0])\n",
    "    n_test_pos = len([x for x in y if x == 1])\n",
    "    n_test_neg = len(y) - n_test_pos\n",
    "    n_test_pos, n_test_neg = str(n_test_pos), str(n_test_neg)\n",
    "    axs[index].imshow(cm, cmap=plt.cm.Blues, interpolation='nearest')\n",
    "    if tp == 'res': tp = 'resampled'\n",
    "    if tp == 'val': tp = 'validation'\n",
    "    axs[index].set_title('{}\\n{} : {}\\naccuracy: {}\\n{} : {}'\n",
    "                         .format(tp.upper(), c, fitur, str(acc), n_test_pos, n_test_neg))\n",
    "    axs[index].set_xlabel('predicted')\n",
    "    axs[index].set_ylabel('actual')\n",
    "    for (k, j), label in np.ndenumerate(cm):\n",
    "        axs[index].text(j, k, label, ha='center', va='center',\n",
    "                        color='red', fontsize=14)\n",
    "\n",
    "def do_training_testing(clf, X, y, tp, show=None):\n",
    "    per_clf = {}\n",
    "    for c in clf: # untuk masing-masing jenis classifier\n",
    "        if show == 'cm': axs = make_subplots(X)\n",
    "        for index, fitur in enumerate(X): # untuk masing-masing jenis fitur\n",
    "            y_train = y[fitur] if tp == 'res' else y\n",
    "            if show == 'cm':\n",
    "                pred = cross_val_predict(clf[c], X[fitur], y_train, cv=10)\n",
    "                per_clf[(c, fitur)] = get_best_model(X[fitur], y_train, clf[c])\n",
    "            if show == 'md': # show process to get best models\n",
    "                print('\\t', c, fitur)\n",
    "                per_clf[(c, fitur)] = get_best_model(X[fitur], y_train, clf[c], verbose=True)\n",
    "            else:\n",
    "                per_clf[(c, fitur)] = get_best_model(X[fitur], y_train, clf[c])\n",
    "            if show == 'cm': show_cm(y_train, pred, axs, index, fitur, c, tp)\n",
    "        if show == 'cm': plt.tight_layout()\n",
    "    return per_clf\n",
    "\n",
    "def do_validation(clf, fitur, per_clf, show=None):\n",
    "    for c in clf:\n",
    "        if show == 'cm': axs = make_subplots(fitur)\n",
    "        total_acc = 0\n",
    "        for index, f in enumerate(fitur):\n",
    "            pred = per_clf[c, f].predict(X_val[f])\n",
    "            gmean_score = round(gmean(y_val, pred, average='binary') * 100, 2)\n",
    "            acc = round(accuracy_score(y_val, pred) * 100, 2)\n",
    "            total_acc += acc\n",
    "            if show == 'cm':\n",
    "                show_cm(y_val, pred, axs, index, f, c, tp='val')\n",
    "            else:\n",
    "                print('acc:gm {} {}: \\t\\t {} : {}'.format(c, f, acc, gmean_score))\n",
    "        avg_acc = round(total_acc / len(fitur), 2)\n",
    "        if show == 'cm':\n",
    "            plt.tight_layout()\n",
    "        else:\n",
    "            print('>> acc rata2 {}: \\t\\t({})'.format(c, avg_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = {\n",
    "       'multi_nb': MultinomialNB(),\n",
    "       'supp_vm': SVC(),\n",
    "       'log_reg': LogisticRegression(),\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do training-testing and validation\n",
    "print the performance result for each algorithms and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============VALIDASI===============\n",
      "\n",
      "no sampling\n",
      "acc:gm multi_nb biner: \t\t 52.0 : 20.0\n",
      "acc:gm multi_nb tf: \t\t 54.0 : 29.85\n",
      "acc:gm multi_nb tfidf: \t\t 52.0 : 22.25\n",
      ">> acc rata2 multi_nb: \t\t(52.67)\n",
      "acc:gm log_reg biner: \t\t 58.0 : 40.0\n",
      "acc:gm log_reg tf: \t\t 58.0 : 41.02\n",
      "acc:gm log_reg tfidf: \t\t 50.0 : 0.0\n",
      ">> acc rata2 log_reg: \t\t(55.33)\n",
      "acc:gm supp_vm biner: \t\t 50.0 : 0.0\n",
      "acc:gm supp_vm tf: \t\t 50.0 : 0.0\n",
      "acc:gm supp_vm tfidf: \t\t 50.0 : 0.0\n",
      ">> acc rata2 supp_vm: \t\t(50.0)\n",
      "selesai dalam 215.41 detik -> 3.59 menit\n",
      "\n",
      "with sampling\n",
      "acc:gm multi_nb biner: \t\t 54.0 : 28.28\n",
      "acc:gm multi_nb tf: \t\t 56.5 : 39.4\n",
      "acc:gm multi_nb tfidf: \t\t 56.0 : 39.19\n",
      ">> acc rata2 multi_nb: \t\t(55.5)\n",
      "acc:gm log_reg biner: \t\t 70.0 : 64.99\n",
      "acc:gm log_reg tf: \t\t 66.0 : 59.77\n",
      "acc:gm log_reg tfidf: \t\t 50.0 : 0.0\n",
      ">> acc rata2 log_reg: \t\t(62.0)\n",
      "acc:gm supp_vm biner: \t\t 52.5 : 22.36\n",
      "acc:gm supp_vm tf: \t\t 69.5 : 69.28\n",
      "acc:gm supp_vm tfidf: \t\t 50.0 : 0.0\n",
      ">> acc rata2 supp_vm: \t\t(57.33)\n",
      "selesai dalam 1437.48 detik -> 23.96 menit\n"
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "print('===============VALIDASI===============\\n')\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "print('no sampling')\n",
    "best_models = do_training_testing(clf, X, y, tp='asli')\n",
    "do_validation(clf, fitur, per_clf=best_models)\n",
    "end = time.time()\n",
    "print('selesai dalam {} detik -> {} menit'.format(round(end-start, 2),\n",
    "                                                  round((end-start)/60, 2)))\n",
    "\n",
    "start = time.time()\n",
    "print('\\nwith sampling')\n",
    "best_models = do_training_testing(clf, X_resampled, y_resampled, tp='res')\n",
    "do_validation(clf, fitur, per_clf=best_models)\n",
    "end = time.time()\n",
    "print('selesai dalam {} detik -> {} menit'.format(round(end-start, 2),\n",
    "                                                  round((end-start)/60, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
